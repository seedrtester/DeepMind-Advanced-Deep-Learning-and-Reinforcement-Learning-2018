{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL1_Q_final.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1GRqxUT0zGus3fN8sE4jZwJGWgazrkO7G",
          "timestamp": 1516294745427
        },
        {
          "file_id": "1yFj1LZ7MPleNtBFX6NS9i1zwZ74huXrQ",
          "timestamp": 1516294640767
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GHl6N3Xfkctm",
        "colab_type": "text"
      },
      "source": [
        "# Homework 1\n",
        "\n",
        "**Start date:** *18th Jan 2017*\n",
        "\n",
        "**Due date:** *04 February 2017, 11:55 pm*\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_DL_hw1.ipynb** before the deadline above.\n",
        "\n",
        "Also send a **sharable link** to the notebook at the following email: ucl.coursework.submit@gmail.com. You can also make it sharable via link to everyone, up to you.\n",
        "\n",
        "### IMPORTANT\n",
        "Please make sure you submission includes **all results/plots/tables** required for grading. We will not re-run your code.\n",
        "\n",
        "## The Data\n",
        "\n",
        "### Handwritten Digit Recognition Dataset (MNIST)\n",
        "\n",
        "In this assignment we will be using the [MNIST digit dataset](https://yann.lecun.com/exdb/mnist/). \n",
        "\n",
        "The dataset contains images of hand-written digits ($0-9$), and the corresponding labels. \n",
        "\n",
        "The images have a resolution of $28\\times 28$ pixels.\n",
        "\n",
        "### The MNIST Dataset in TensorFlow\n",
        "\n",
        "You can use the tensorflow build-in functionality to download and import the dataset into python (see *Setup* section below).\n",
        "\n",
        "## The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use TensorFlow to implement several neural network models (labelled Model 1-4, and described in the corresponding sections of the Colab).\n",
        "\n",
        "You will then train these models to classify hand written digits from the Mnist dataset.\n",
        "\n",
        "### Variable Initialization\n",
        "\n",
        "Initialize the variables containing the parameters using [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a.html).\n",
        "\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "    my_variable = tf.Variable(initializer(shape))\n",
        "\n",
        "### Hyper-parameters\n",
        "\n",
        "For each of these models you will be requested to run experiments with different hyper-parameters.\n",
        "\n",
        "More specifically, you will be requested to try 3 sets of hyper-parameters per model, and report the resulting model accuracy.\n",
        "\n",
        "Each combination of hyper-parameter will specify how to set each of the following:\n",
        "\n",
        "- **num_epochs**: Number of iterations through the training section of the dataset [*a positive integer*].\n",
        "\n",
        "- **learning_rate**: Learning rate used by the gradient descent optimizer [*a scalar between 0 and 1*]\n",
        "\n",
        "In all experiments use a *batch_size* of 100.\n",
        "\n",
        "### Loss function\n",
        "All models, should be trained as to minimize the **cross-entropy loss** function:\n",
        "$$\n",
        "\\mathrm{loss}\n",
        "~~=~~\n",
        "-\\sum_{i=1}^N \\log p(y_i|x_i, \\theta)\n",
        "~~=~~\n",
        "-\\sum_{i=1}^N \\log{ \\underbrace{\\left(\\frac{\\exp(z_{i}[y_i])}{\\sum_{c=1}^{10} \\exp(z_{i}[c])}\\right)}_{\\text{softmax output}}}\n",
        "~~=~~\n",
        "\\sum_{i=1}^N \\left( -z_{i}[y_i] + \\log{\\left( \\sum_{c=1}^{10} \\exp(z_{i}[c]) \\right)} \\right)$$\n",
        "where $z \\in \\mathbb{R}^{10}$ is the input to the softmax layer and $z{[c]}$ denotes the $c$-th entry of vector $z$. And $i$ is a index for the dataset $\\{(x_i, y_i)\\}_{i=1}^N$.\n",
        "\n",
        "*Note*: Sum the loss across the elements of the batch with tf.reduce_sum().\n",
        "\n",
        "*Hint*: read about TensorFlow's [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) function.\n",
        "\n",
        "### Optimization\n",
        "\n",
        "Use **stochastic gradient descent (SGD)** for optimizing the loss function.\n",
        "\n",
        "Hint: read about TensorFlow's [tf.train.GradientDescentOptimizer()](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer).\n",
        "\n",
        "\n",
        "### Training and Evaluation\n",
        "\n",
        "The tensorflow built-in functionality for downloading and importing the dataset into python returns a Datasets object.\n",
        "\n",
        "This object will have three attributes: \n",
        "\n",
        "- train\n",
        "\n",
        "- validation\n",
        "\n",
        "- test\n",
        "\n",
        "Use only the **train** data in order to optimize the model.\n",
        "\n",
        "Use *datasets.train.next_batch(100)* in order to sample mini-batches of data.\n",
        "\n",
        "Every 20000 training samples (i.e. every 200 updates to the model), interrupt training and measure the accuracy of the model, \n",
        "\n",
        "each time evaluate the accuracy of the model both on 20% of the **train** set and on the entire **test** set.\n",
        "\n",
        "### Reporting\n",
        "\n",
        "For each model i, you will collect the learning curves associated to each combination of hyper-parameters.\n",
        "\n",
        "Use the utility function `plot_learning_curves` to plot these learning curves,\n",
        "\n",
        "and the and utility function `plot_summary_table` to generate a summary table of results.\n",
        "\n",
        "For each run collect the train and test curves in a tuple, together with the hyper-parameters.\n",
        "\n",
        "    experiments_task_i = [\n",
        "\n",
        "       (num_epochs_1, learning_rate_1), train_accuracy_1, test_accuracy_1),\n",
        "    \n",
        "       (num_epochs_2, learning_rate_2), train_accuracy_2, test_accuracy_2),\n",
        "    \n",
        "       (num_epochs_3, learning_rate_3), train_accuracy_3, test_accuracy_3)]\n",
        "\n",
        "### Hint \n",
        "\n",
        "If you need some extra help, familiarizing yourselves with the dataset and the task of building models in TensorFlow, you can check the [TF tutorial for MNIST](https://www.tensorflow.org/tutorials/mnist/beginners/). \n",
        "\n",
        "The tutorial will walk you through the MNIST classification task step-by-step, building and optimizing a model in TensorFlow. \n",
        "\n",
        "(Please do not copy the provided code, though. Walk through the tutorial, but write your own implementation)."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "0wg-OYIMm8cW",
        "colab_type": "text"
      },
      "source": [
        "# Imports and utility functions (do not modify!)"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "rPYnfzQfltxp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Import useful libraries.\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Global variables.\n",
        "log_period_samples = 20000\n",
        "batch_size = 100\n",
        "\n",
        "# Import dataset with one-hot encoding of the class labels.\n",
        "def get_data():\n",
        "  return input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# Placeholders to feed train and test data into the graph.\n",
        "# Since batch dimension is 'None', we can reuse them both for train and eval.\n",
        "def get_placeholders():\n",
        "  x = tf.placeholder(tf.float32, [None, 784])\n",
        "  y_ = tf.placeholder(tf.float32, [None, 10])\n",
        "  return x, y_\n",
        "\n",
        "# Plot learning curves of experiments\n",
        "def plot_learning_curves(experiment_data):\n",
        "  # Generate figure.\n",
        "  fig, axes = plt.subplots(3, 4, figsize=(22,12))\n",
        "  st = fig.suptitle(\n",
        "      \"Learning Curves for all Tasks and Hyper-parameter settings\",\n",
        "      fontsize=\"x-large\")\n",
        "  # Plot all learning curves.\n",
        "  for i, results in enumerate(experiment_data):\n",
        "    for j, (setting, train_accuracy, test_accuracy) in enumerate(results):\n",
        "      # Plot.\n",
        "      xs = [x * log_period_samples for x in range(1, len(train_accuracy)+1)]\n",
        "      axes[j, i].plot(xs, train_accuracy, label='train_accuracy')\n",
        "      axes[j, i].plot(xs, test_accuracy, label='test_accuracy')\n",
        "      # Prettify individual plots.\n",
        "      axes[j, i].ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
        "      axes[j, i].set_xlabel('Number of samples processed')\n",
        "      axes[j, i].set_ylabel('Epochs: {}, Learning rate: {}.  Accuracy'.format(*setting))\n",
        "      axes[j, i].set_title('Task {}'.format(i + 1))\n",
        "      axes[j, i].legend()\n",
        "  # Prettify overall figure.\n",
        "  plt.tight_layout()\n",
        "  st.set_y(0.95)\n",
        "  fig.subplots_adjust(top=0.91)\n",
        "  plt.show()\n",
        "\n",
        "# Generate summary table of results.\n",
        "def plot_summary_table(experiment_data):\n",
        "  # Fill Data.\n",
        "  cell_text = []\n",
        "  rows = []\n",
        "  columns = ['Setting 1', 'Setting 2', 'Setting 3']\n",
        "  for i, results in enumerate(experiment_data):\n",
        "    rows.append('Model {}'.format(i + 1))\n",
        "    cell_text.append([])\n",
        "    for j, (setting, train_accuracy, test_accuracy) in enumerate(results):\n",
        "      cell_text[i].append(test_accuracy[-1])\n",
        "  # Generate Table.\n",
        "  fig=plt.figure(frameon=False)\n",
        "  ax = plt.gca()\n",
        "  the_table = ax.table(\n",
        "      cellText=cell_text,\n",
        "      rowLabels=rows,\n",
        "      colLabels=columns,\n",
        "      loc='center')\n",
        "  the_table.scale(1, 4)\n",
        "  # Prettify.\n",
        "  ax.patch.set_facecolor('None')\n",
        "  ax.xaxis.set_visible(False)\n",
        "  ax.yaxis.set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-52fO1Axmd9B",
        "colab_type": "text"
      },
      "source": [
        "# Model 1 (20 pts)\n",
        "\n",
        "### Model\n",
        "\n",
        "Train a neural network model consisting of 1 linear layer, followed by a softmax:\n",
        "\n",
        "(input $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
        "\n",
        "### Hyper-parameters\n",
        "\n",
        "Train the model with three different hyper-parameter settings:\n",
        "\n",
        "- *num_epochs*=5, *learning_rate*=0.0001\n",
        "\n",
        "- *num_epochs*=5, *learning_rate*=0.005\n",
        "\n",
        "- *num_epochs*=5, *learning_rate*=0.1"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "iIT9lwPPmpbO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.\n",
        "# Store results of runs with different configurations in a dictionary.\n",
        "# Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)\n",
        "experiments_task1 = []\n",
        "settings = [(5, 0.0001), (5, 0.005), (5, 0.1)]"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9t84PdvyBmP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "print('Training Model 1')\n",
        "\n",
        "# Train Model 1 with the different hyper-parameter settings.\n",
        "for (num_epochs, learning_rate) in settings:\n",
        "\n",
        "  # Reset graph, recreate placeholders and dataset.\n",
        "  tf.reset_default_graph()\n",
        "  x, y_ = get_placeholders()\n",
        "  mnist = get_data()\n",
        "  eval_mnist = get_data()\n",
        "\n",
        "  #####################################################\n",
        "  # Define model, loss, update and evaluation metric. #\n",
        "  #####################################################\n",
        "\n",
        "  # Train.\n",
        "  i, train_accuracy, test_accuracy = 0, [], []\n",
        "  log_period_updates = int(log_period_samples / batch_size)\n",
        "  with tf.train.MonitoredSession() as sess:\n",
        "    while mnist.train.epochs_completed < num_epochs:\n",
        "      \n",
        "      # Update.\n",
        "      i += 1\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "      \n",
        "      #################\n",
        "      # Training step #\n",
        "      #################\n",
        "      \n",
        "      # Periodically evaluate.\n",
        "      if i % log_period_updates == 0:\n",
        "        \n",
        "        #####################################\n",
        "        # Compute and store train accuracy. #\n",
        "        #####################################\n",
        "        \n",
        "        #####################################\n",
        "        # Compute and store test accuracy.  #\n",
        "        #####################################\n",
        "\n",
        "  experiments_task1.append(\n",
        "      ((num_epochs, learning_rate), train_accuracy, test_accuracy))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7jHmjyz_jFr9",
        "colab_type": "text"
      },
      "source": [
        "# Model 2 (20 pts)\n",
        "\n",
        "1 hidden layer (32 units) with a ReLU non-linearity, followed by a softmax.\n",
        "\n",
        "(input $\\rightarrow$ non-linear layer $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
        "\n",
        "### Hyper-parameters\n",
        "\n",
        "Train the model with three different hyper-parameter settings:\n",
        "\n",
        "- *num_epochs*=15, *learning_rate*=0.0001\n",
        "\n",
        "- *num_epochs*=15, *learning_rate*=0.005\n",
        "\n",
        "- *num_epochs*=15, *learning_rate*=0.1 "
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "oGAT8ewLeyt_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.\n",
        "# Store results of runs with different configurations in a dictionary.\n",
        "# Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)\n",
        "experiments_task2 = []\n",
        "settings = [(15, 0.0001), (15, 0.005), (15, 0.1)]"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-JjlA-Wh3hS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "print('Training Model 2')\n",
        "\n",
        "# Train Model 1 with the different hyper-parameter settings.\n",
        "for (num_epochs, learning_rate) in settings:\n",
        "  \n",
        "  # Reset graph, recreate placeholders and dataset.\n",
        "  tf.reset_default_graph()  # reset the tensorflow graph\n",
        "  x, y_ = get_placeholders()\n",
        "  mnist = get_data()  # use for training.\n",
        "  eval_mnist = get_data()  # use for evaluation.\n",
        "\n",
        "  #####################################################\n",
        "  # Define model, loss, update and evaluation metric. #\n",
        "  #####################################################\n",
        "\n",
        "  # Train.\n",
        "  i, train_accuracy, test_accuracy = 0, [], []\n",
        "  log_period_updates = int(log_period_samples / batch_size)\n",
        "  with tf.train.MonitoredSession() as sess:\n",
        "    while mnist.train.epochs_completed < num_epochs:\n",
        "\n",
        "      # Update.\n",
        "      i += 1\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      #################\n",
        "      # Training step #\n",
        "      #################\n",
        "\n",
        "      # Periodically evaluate.\n",
        "      if i % log_period_updates == 0:\n",
        "\n",
        "        #####################################\n",
        "        # Compute and store train accuracy. #\n",
        "        #####################################\n",
        "        \n",
        "        #####################################\n",
        "        # Compute and store test accuracy.  #\n",
        "        #####################################\n",
        "\n",
        "    experiments_task2.append(\n",
        "        ((num_epochs, learning_rate), train_accuracy, test_accuracy))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ku6yREFUjPM2",
        "colab_type": "text"
      },
      "source": [
        "# Model 3 (20 pts)\n",
        "\n",
        "2 hidden layers (32 units) each, with ReLU non-linearity, followed by a softmax.\n",
        "\n",
        "(input $\\rightarrow$ non-linear layer $\\rightarrow$ non-linear layer $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
        "\n",
        "\n",
        "### Hyper-parameters\n",
        "\n",
        "Train the model with three different hyper-parameter settings:\n",
        "\n",
        "- *num_epochs*=5, *learning_rate*=0.003\n",
        "\n",
        "- *num_epochs*=40, *learning_rate*=0.003\n",
        "\n",
        "- *num_epochs*=40, *learning_rate*=0.05"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "vGzx1NypyMi6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.\n",
        "# Store results of runs with different configurations in a dictionary.\n",
        "# Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)\n",
        "experiments_task3 = []\n",
        "settings = [(5, 0.003), (40, 0.003), (40, 0.05)]"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z16Nvf35yMlR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "print('Training Model 3')\n",
        "\n",
        "# Train Model 1 with the different hyper-parameter settings.\n",
        "for (num_epochs, learning_rate) in settings:\n",
        "  \n",
        "  # Reset graph, recreate placeholders and dataset.\n",
        "  tf.reset_default_graph()  # reset the tensorflow graph\n",
        "  x, y_ = get_placeholders()\n",
        "  mnist = get_data()  # use for training.\n",
        "  eval_mnist = get_data()  # use for evaluation.\n",
        "\n",
        "  #####################################################\n",
        "  # Define model, loss, update and evaluation metric. #\n",
        "  #####################################################\n",
        "\n",
        "  # Train.\n",
        "  i, train_accuracy, test_accuracy = 0, [], []\n",
        "  log_period_updates = int(log_period_samples / batch_size)\n",
        "  with tf.train.MonitoredSession() as sess:\n",
        "    while mnist.train.epochs_completed < num_epochs:\n",
        "\n",
        "      # Update.\n",
        "      i += 1\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      #################\n",
        "      # Training step #\n",
        "      #################\n",
        "\n",
        "      # Periodically evaluate.\n",
        "      if i % log_period_updates == 0:\n",
        "\n",
        "        #####################################\n",
        "        # Compute and store train accuracy. #\n",
        "        #####################################\n",
        "        \n",
        "        #####################################\n",
        "        # Compute and store test accuracy.  #\n",
        "        #####################################\n",
        "\n",
        "    experiments_task3.append(\n",
        "        ((num_epochs, learning_rate), train_accuracy, test_accuracy))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XdldRURrjiAd",
        "colab_type": "text"
      },
      "source": [
        "# Model 4 (20 pts)\n",
        "\n",
        "### Model\n",
        "3 layer convolutional model (2 convolutional layers followed by max pooling) + 1 non-linear layer (32 units), followed by softmax. \n",
        "\n",
        "(input(28x28) $\\rightarrow$ conv(3x3x8) + maxpool(2x2) $\\rightarrow$ conv(3x3x8) + maxpool(2x2) $\\rightarrow$ flatten $\\rightarrow$ non-linear $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
        "\n",
        "\n",
        "- Use *padding = 'SAME'* for both the convolution and the max pooling layers. \n",
        "\n",
        "- Employ plain convolution (no stride) and for max pooling operations use 2x2 sliding windows, with no overlapping pixels (note: this operation will down-sample the input image by 2x2).\n",
        "\n",
        "### Hyper-parameters\n",
        "\n",
        "Train the model with three different hyper-parameter settings:\n",
        "\n",
        "- num_epochs=5, learning_rate=0.01\n",
        "\n",
        "- num_epochs=10, learning_rate=0.001\n",
        "\n",
        "- num_epochs=20, learning_rate=0.001 \n"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "TOUd73i2yQS6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# CAREFUL: Running this CL resets the experiments_task1 dictionary where results should be stored.\n",
        "# Store results of runs with different configurations in a dictionary.\n",
        "# Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)\n",
        "experiments_task4 = []\n",
        "settings = [(5, 0.01), (10, 0.001), (20, 0.001)]"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFtzPQWGySM3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "print('Training Model 4')\n",
        "\n",
        "# Train Model 1 with the different hyper-parameter settings.\n",
        "for (num_epochs, learning_rate) in settings:\n",
        "  \n",
        "  # Reset graph, recreate placeholders and dataset.\n",
        "  tf.reset_default_graph()  # reset the tensorflow graph\n",
        "  x, y_ = get_placeholders()\n",
        "  x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "  mnist = get_data()  # use for training.\n",
        "  eval_mnist = get_data()  # use for evaluation.\n",
        "\n",
        "  #####################################################\n",
        "  # Define model, loss, update and evaluation metric. #\n",
        "  #####################################################\n",
        "\n",
        "  # Train.\n",
        "  i, train_accuracy, test_accuracy = 0, [], []\n",
        "  log_period_updates = int(log_period_samples / batch_size)\n",
        "  with tf.train.MonitoredSession() as sess:\n",
        "    while mnist.train.epochs_completed < num_epochs:\n",
        "\n",
        "      # Update.\n",
        "      i += 1\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      #################\n",
        "      # Training step #\n",
        "      #################\n",
        "\n",
        "      # Periodically evaluate.\n",
        "      if i % log_period_updates == 0:\n",
        "\n",
        "        #####################################\n",
        "        # Compute and store train accuracy. #\n",
        "        #####################################\n",
        "        \n",
        "        #####################################\n",
        "        # Compute and store test accuracy.  #\n",
        "        #####################################\n",
        "\n",
        "    experiments_task4.append(\n",
        "        ((num_epochs, learning_rate), train_accuracy, test_accuracy))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4mQKAVGkezs",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "fW1JBX_XCs4-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "plot_learning_curves([experiments_task1, experiments_task2, experiments_task3, experiments_task4])"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hT0JYzKt8PgH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "plot_summary_table([experiments_task1, experiments_task2, experiments_task3, experiments_task4])"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ql4MBS73k-F-",
        "colab_type": "text"
      },
      "source": [
        "# Questions"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "z8IVSIR3P1Xh",
        "colab_type": "text"
      },
      "source": [
        "### Q1 (5 pts): Indicate which of the previous experiments constitute an example of over-fitting. Why is this happening?"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "NWYk2iQZQKwV",
        "colab_type": "text"
      },
      "source": [
        "### Q2 (5 pts): Indicate which of the previous experiments constitute an example of under-fitting. Why is this happening?"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "vxw5ItL_QVRg",
        "colab_type": "text"
      },
      "source": [
        "### Q3 (10 pts): How would you prevent over-/under-fitting from happening?"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "K9ZBONFgkJpb",
        "colab_type": "text"
      },
      "source": [
        "# Extension (Ungraded)\n",
        "\n",
        "In the previous tasks you have used plain Stohastic Gradient Descent to train the models.\n",
        "\n",
        "There is a large literatures on variants of Stochastic Gradient Descent, that improve learning speed and robustness to hyper-parameters.\n",
        "\n",
        "[Here](https://www.tensorflow.org/api_docs/python/train/optimizers) you can find the documentation for several optimizers already implemented in TensorFlow, as well as the original papers proposing these methods.*italicized text*.\n",
        "\n",
        "AdamOptimizer and RMSProp are among the most commonly employed in Deep Learning.\n",
        "\n",
        "How does replacing SGD with these optimizers affect the previous results?"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "S9F28g2Xy7mK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Feel free to experiment!"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juVwDBIwWb9v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        ""
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    }
  ]
}